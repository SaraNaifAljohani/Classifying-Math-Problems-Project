# -*- coding: utf-8 -*-
"""Classifying-Math-Problems

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gPmFFqEaw0EX2TGFkti0aN3oAIRCD7n2

# Classifying Math Problems

**Approach:** Fine-tuning BERT for 8-class NLP classification using HuggingFace Transformers.

**Tools:** Python, Colab, HuggingFace Transformers & Datasets, PyTorch, scikit-learn.

**Goal:** Build a model to classify math questions into one of 8 problem types (e.g., Algebra, Geometry, etc.)
"""

import os
!pip install datasets # Install HuggingFace datasets library for easier integration with Trainer API

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from datasets import Dataset
from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer
import torch
from sklearn.metrics import accuracy_score

df = pd.read_csv("/content/train.csv")

# Encoding string labels to numbers (e.g., Algebra = 0, Geometry = 1, Arithmetic = 2):
label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['label'])

# Splitting data into training and validation sets (80/20 split):
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)
train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)

# Converting text into tokens, smaller units that a model can understand:
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def tokenize_function(examples):
    return tokenizer(examples["Question"], padding="max_length", truncation=True)

train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)

train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# Loading a pre-trained BERT model and modify the classification head for 8 output classes:
num_labels = len(label_encoder.classes_)
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=num_labels)


# Setting up how training should behave, balancing performance and runtime:
training_args = TrainingArguments(
    output_dir="./results",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Metrics:
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = torch.argmax(torch.tensor(logits), dim=1)
    return {"accuracy": accuracy_score(labels, preds)}


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()

results = trainer.evaluate()
print(f"Accuracy: {results['eval_accuracy']:.4f}")

test_df = pd.read_csv("/content/test.csv")
test_encodings = tokenizer(test_df["Question"].tolist(), truncation=True, padding=True)
test_dataset = Dataset.from_dict(test_encodings)

predictions = trainer.predict(test_dataset)
predicted_labels = predictions.predictions.argmax(axis=-1)

# Making sure the file match the required format:
submission_df = pd.DataFrame({
    "id": test_df["id"],
    "label": predicted_labels
})

submission_df.to_csv("submission.csv", index=False)